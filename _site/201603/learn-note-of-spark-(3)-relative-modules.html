<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8">
  
  <title>Spark学习手册（三）：Spark模块学习摘读 | 桃子的博客铭</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Spark在其基础架构之上支持四大模块，分别是SparkSQL、SparkStreaming、MLlib和GraphX，本文将对这几个模块的手册进行阅读摘录。

一、Spark SQL1.1 简介　　同Spark SQL的交互方式包括SQL、DataFrames API和Datasets API，但是其内部的执行引擎是一样的，只是对外表现的接口不一样而已。

SQL：可以使用基础的SQL语法或">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习手册（三）：Spark模块学习摘读">
<meta property="og:url" content="https://taozj.org/201603/learn-note-of-spark-(3)-relative-modules.html">
<meta property="og:site_name" content="桃子的博客铭">
<meta property="og:description" content="Spark在其基础架构之上支持四大模块，分别是SparkSQL、SparkStreaming、MLlib和GraphX，本文将对这几个模块的手册进行阅读摘录。

一、Spark SQL1.1 简介　　同Spark SQL的交互方式包括SQL、DataFrames API和Datasets API，但是其内部的执行引擎是一样的，只是对外表现的接口不一样而已。

SQL：可以使用基础的SQL语法或">
<meta property="og:image" content="https://taozj.org/post_images/images/201603/cf8659d1067cbe8d5f3ec06f1a44f3d0.png">
<meta property="og:updated_time" content="2016-12-18T14:06:06.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark学习手册（三）：Spark模块学习摘读">
<meta name="twitter:description" content="Spark在其基础架构之上支持四大模块，分别是SparkSQL、SparkStreaming、MLlib和GraphX，本文将对这几个模块的手册进行阅读摘录。

一、Spark SQL1.1 简介　　同Spark SQL的交互方式包括SQL、DataFrames API和Datasets API，但是其内部的执行引擎是一样的，只是对外表现的接口不一样而已。

SQL：可以使用基础的SQL语法或">
<meta name="twitter:image" content="https://taozj.org/post_images/images/201603/cf8659d1067cbe8d5f3ec06f1a44f3d0.png">
  
    <link rel="alternative" href="/atom.xml" title="桃子的博客铭" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico">
  
  
      <link href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css" rel="stylesheet">
  
  
      <link href="//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css" rel="stylesheet">
  
  
      <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
      <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">
  
  <link rel="stylesheet" href="/css/style.css">
  
  <link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="/css/iconfont.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">
  
  <script src="//cdn.bootcss.com/jquery/1.9.1/jquery.min.js"></script>
  <script src="//cdn.bootcss.com/clipboard.js/1.5.9/clipboard.min.js"></script>
  <script>
      var yiliaConfig = {
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: true,
          fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
          scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.0.9/scrollreveal.min.js"
      }
  </script>

  
      <script>
          yiliaConfig.rootUrl = "/";
      </script>
  

  
  
</head>

<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/post_images/resources/avatar.jpeg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">taozj</a></h1>
        </hgroup>

        
        <p class="header-subtitle">高性能、高可用服务端开发</p>
        
        <br>
        
        <p class="header-subtitle">淡泊明志，宁静致远！</p>
        
                


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/archives">文章归档</a></li>
                        
                            <li><a href="https://album.taozj.org">个人相册</a></li>
                        
                            <li><a href="https://wiki.taozj.org/doku.php">个人WiKi</a></li>
                        
                            <li><a href="/about">请你读我</a></li>
                        
                            <li><a href="/search.html">搜索本站</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa GitHub" target="_blank" href="https://github.com/taozhijiang" title="GitHub"></a>
                            
                                <a class="fa 新浪微博" target="_blank" href="http://weibo.com/u/1683951363" title="新浪微博"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa Email" target="_blank" href="mailto:dEB0YW96ai5vcmcK (base64)" title="Email"></a>
                            
                                <a class="fa QQ" target="_blank" href="/347714173" title="QQ"></a>
                            
                                <a class="fa Twitter" target="_blank" href="https://twitter.com/taozhijiang" title="Twitter"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C-与Boost/">C++与Boost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux开发/">Linux开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/互联网/">互联网</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/内核/">内核</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分布式/">分布式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/协议/">协议</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/博客/">博客</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/后台开发/">后台开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/工作/">工作</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/工作相关/">工作相关</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/开发/">开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/开发基础/">开发基础</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数学/">数学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据结构和算法/">数据结构和算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/服务运维/">服务运维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/构架/">构架</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/架构/">架构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/生活杂感/">生活杂感</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/读书笔记/">读书笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/软件/">软件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/运维/">运维</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/造轮子/">造轮子</a></li></ul>
                    </div>
                </section>
                
                
                

                
            </div>
        </div>
    </header>                
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">taozj</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/post_images/resources/avatar.jpeg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">taozj</a></h1>
            </hgroup>
            
            <p class="header-subtitle">高性能、高可用服务端开发</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/archives">文章归档</a></li>
                
                    <li><a href="https://album.taozj.org">个人相册</a></li>
                
                    <li><a href="https://wiki.taozj.org/doku.php">个人WiKi</a></li>
                
                    <li><a href="/about">请你读我</a></li>
                
                    <li><a href="/search.html">搜索本站</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa GitHub" target="_blank" href="https://github.com/taozhijiang" title="GitHub"></a>
                            
                                <a class="fa 新浪微博" target="_blank" href="http://weibo.com/u/1683951363" title="新浪微博"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa Email" target="_blank" href="mailto:dEB0YW96ai5vcmcK (base64)" title="Email"></a>
                            
                                <a class="fa QQ" target="_blank" href="/347714173" title="QQ"></a>
                            
                                <a class="fa Twitter" target="_blank" href="https://twitter.com/taozhijiang" title="Twitter"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-learn-note-of-spark-(3)-relative-modules" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/201603/learn-note-of-spark-(3)-relative-modules.html" class="article-date">
      <time datetime="2016-03-29T10:46:36.000Z" itemprop="datePublished">2016-03-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark学习手册（三）：Spark模块学习摘读
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/开发基础/">开发基础</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/工作相关/">工作相关</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/开发基础/">开发基础</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/读书笔记/">读书笔记</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/软件/">软件</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>　　Spark在其基础架构之上支持四大模块，分别是SparkSQL、SparkStreaming、MLlib和GraphX，本文将对这几个模块的手册进行阅读摘录。</p>
<p><img src="/post_images/images/201603/cf8659d1067cbe8d5f3ec06f1a44f3d0.png" alt="Spark 模块"></p>
<h1 id="一、Spark_SQL">一、Spark SQL</h1><h2 id="1-1_简介">1.1 简介</h2><p>　　同Spark SQL的交互方式包括SQL、DataFrames API和Datasets API，但是其内部的执行引擎是一样的，只是对外表现的接口不一样而已。</p>
<ul>
<li>SQL：可以使用基础的SQL语法或HiveQL，当在别的编程语言中执行SQL，返回的是DataFrame格式的结果。SQL的交互方式包括命令行方式，以及JDBC、ODBC对数据库的访问接口。</li>
<li>DataFrame：一种通过命名列组织的分布式数据存储，概念上和关系数据库的表等价，可以接受文件、数据库、RDD等数据源来创建。</li>
<li>Datasets：暂不支持Python，没研究。</li>
</ul>
<h2 id="1-2_从文件创建DataFrame">1.2 从文件创建DataFrame</h2><p>　　基础SQLContext环境创建，并从json文件创建DataFrame<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>sqlContext = SQLContext(sc)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df = sqlContext.read.json(<span class="string">"/root/spark-1.6.1-bin-hadoop2.6/examples/src/main/resources/people.json"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df.show()</div><div class="line">+----+-------+</div><div class="line">| age|   name|</div><div class="line">+----+-------+</div><div class="line">|null|Michael|</div><div class="line">|  <span class="number">30</span>|   Andy|</div><div class="line">|  <span class="number">19</span>| Justin|</div><div class="line">+----+-------+</div></pre></td></tr></table></figure></p>
<p>DataFrame的常用操作接口(看意思就明白的)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>df.printSchema()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(<span class="string">"name"</span>).show()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(df[<span class="string">'name'</span>], df[<span class="string">'age'</span>] + <span class="number">1</span>).show()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df.filter(df[<span class="string">'age'</span>] &gt; <span class="number">21</span>).show()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df.groupBy(<span class="string">"age"</span>).count().show()</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h2 id="1-3_从RDD创建DataFrame">1.3 从RDD创建DataFrame</h2><p>　　将已有的RDD转换为DataFrame有两种方法：一种是映射(reflection)；另外一种是通过变成接口创建表，然后将RDD数据应用到表上面去</p>
<ul>
<li><p>Reflection：通过创建(table_column_name, type)的Row对象，然后通过map将RDD每一行转换成Row，简单但是不够灵活</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext, Row</div><div class="line">sqlContext = SQLContext(sc)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>lines = sc.textFile(<span class="string">"/root/spark-1.6.1-bin-hadoop2.6/examples/src/main/resources/people.txt"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>parts = lines.map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(people.collect())</div><div class="line">[[<span class="string">'Michael'</span>, <span class="string">' 29'</span>], [<span class="string">'Andy'</span>, <span class="string">' 30'</span>], [<span class="string">'Justin'</span>, <span class="string">' 19'</span>]]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>people = parts.map(<span class="keyword">lambda</span> p: Row(name=p[<span class="number">0</span>], age=int(p[<span class="number">1</span>])))</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(people.collect())</div><div class="line">[Row(age=<span class="number">29</span>, name=<span class="string">'Michael'</span>), Row(age=<span class="number">30</span>, name=<span class="string">'Andy'</span>), Row(age=<span class="number">19</span>, name=<span class="string">'Justin'</span>)]</div><div class="line"></div><div class="line"><span class="comment"># 创建schema，并注册成表</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>schemaPeople = sqlContext.createDataFrame(people)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>schemaPeople.registerTempTable(<span class="string">"people"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>schemaPeople.printSchema()</div><div class="line">root</div><div class="line"> |-- age: long (nullable = true)</div><div class="line"> |-- name: string (nullable = true)</div><div class="line"></div><div class="line"><span class="comment"># 访问表内容</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>teenagers = sqlContext.sql(<span class="string">"SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(teenagers.collect())  <span class="comment"># teenagers 是RDD的类型，可以常规的访问</span></div><div class="line">[Row(name=<span class="string">'Justin'</span>)]</div></pre></td></tr></table></figure>
</li>
<li><p>编程的方式创建<br>　　这种一般是事先不知道表结构，比如传递过来动态解析的表结构的情况下，就只能这么操作了。<br>下面的例子可以显示出来，其中列名是schemaString是通过动态的字符串创建的，所以在实际使用中可以各种方式指定，灵活性比较的强</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext, Row</div><div class="line">sqlContext = SQLContext(sc)</div><div class="line"></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>lines = sc.textFile(<span class="string">"/root/spark-1.6.1-bin-hadoop2.6/examples/src/main/resources/people.txt"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>parts = lines.map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>people = parts.map(<span class="keyword">lambda</span> p: Row(name=p[<span class="number">0</span>], age=int(p[<span class="number">1</span>])))</div><div class="line"></div><div class="line"><span class="comment"># 创建schema string</span></div><div class="line">schemaString = <span class="string">"name age"</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>fields = [StructField(field_name, StringType(), <span class="keyword">True</span>) <span class="keyword">for</span> field_name <span class="keyword">in</span> schemaString.split()]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>schema = StructType(fields)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(fields)</div><div class="line">[StructField(name,StringType,true), StructField(age,StringType,true)]</div><div class="line"></div><div class="line"><span class="comment"># 将创建的schema应用到people数据上去</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>schemaPeople = sqlContext.createDataFrame(people, schema)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>schemaPeople.registerTempTable(<span class="string">"people"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>schemaPeople.printSchema()</div><div class="line">root</div><div class="line"> |-- name: string (nullable = true)</div><div class="line"> |-- age: string (nullable = true)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>results = sqlContext.sql(<span class="string">"SELECT name FROM people"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(results.collect())</div><div class="line">[Row(name=<span class="string">'29'</span>), Row(name=<span class="string">'30'</span>), Row(name=<span class="string">'19'</span>)]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df2 = sqlContext.read.load(<span class="string">"/root/namesAndAgesData.parquet"</span>)  <span class="comment">#default is parquet</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>其中，save支持额外的SaveMode参数，可选的值为”error”、”append”、”overwrite”、”ignore”，类似于普通文件接口，他们在目标文件存在的时候，表现为不同的形式。</p>
<h2 id="1-4_Data_Sources">1.4 Data Sources</h2><ul>
<li><p>文件的加载和保存<br>这里是通用方式的文件加载和保存，意味着parquet和json都能使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>df = sqlContext.read.load(<span class="string">"/root/spark-1.6.1-bin-hadoop2.6/examples/src/main/resources/people.json"</span>, format=<span class="string">"json"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(df.collect())</div><div class="line">[Row(age=<span class="keyword">None</span>, name=<span class="string">'Michael'</span>), Row(age=<span class="number">30</span>, name=<span class="string">'Andy'</span>), Row(age=<span class="number">19</span>, name=<span class="string">'Justin'</span>)]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df.select(<span class="string">"name"</span>, <span class="string">"age"</span>).write.save(<span class="string">"namesAndAgesData.parquet"</span>, format=<span class="string">"parquet"</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>编程方式加载和访问parquet格式文件的例子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>parquetFile = sqlContext.read.parquet(<span class="string">"/root/namesAndAgesData.parquet"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>parquetFile.registerTempTable(<span class="string">"parquetFile"</span>); <span class="comment">#可以注册为临时表</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>teenagers = sqlContext.sql(<span class="string">"SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 39"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(teenagers.collect())</div><div class="line">[Row(name=<span class="string">'Justin'</span>), Row(name=<span class="string">'Andy'</span>)]</div></pre></td></tr></table></figure>
</li>
<li><p>Json文件格式<br>同上面的parquet一样的，只是对应的接口改成了sqlContext.read.json。同时，json还支持如下方式创建RDD：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>anotherPeopleRDD = sc.parallelize([\</div><div class="line"><span class="meta">... </span><span class="string">'&#123;"name":"Yin1","address":&#123;"city1":"Columbus","state":"Ohios"&#125;&#125;'</span>,\</div><div class="line"><span class="meta">... </span><span class="string">'&#123;"name":"Yin2","address":&#123;"city2":"Columbus","state":"Ohiot"&#125;&#125;'</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(anotherPeopleRDD.collect())</div><div class="line">[<span class="string">'&#123;"name":"Yin1","address":&#123;"city1":"Columbus","state":"Ohios"&#125;&#125;'</span>, <span class="string">'&#123;"name":"Yin2","address":&#123;"city2":"Columbus","state":"Ohiot"&#125;&#125;'</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>anotherPeople = sqlContext.jsonRDD(anotherPeopleRDD)</div><div class="line">[Row(address=Row(city1=<span class="string">'Columbus'</span>, city2=<span class="keyword">None</span>, city3=<span class="keyword">None</span>, state=<span class="string">'Ohios'</span>), name=<span class="string">'Yin1'</span>), Row(address=Row(city1=<span class="keyword">None</span>, city2=<span class="string">'Columbus'</span>, city3=<span class="keyword">None</span>, state=<span class="string">'Ohiot'</span>), name=<span class="string">'Yin2'</span>)]</div></pre></td></tr></table></figure>
</li>
<li><p>JDBC/ODBC访问<br>启动的时候，需要指定JDBC的驱动，这个驱动可以在mysql的官方网站下载</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">root@ubuntu1404-node01:~/spark-1.6.1-bin-hadoop2.6<span class="comment"># PYSPARK_PYTHON=python3.4 SPARK_CLASSPATH=mysql-connector-java-5.1.38-bin.jar bin/pyspark</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>然后，就可以操作数据库了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> *</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>sqlContext = SQLContext(sc)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df = sqlContext.read.format(<span class="string">'jdbc'</span>).options(url=<span class="string">'jdbc:mysql://113.106.94.201:3306/n_xxxxx'</span>, dbtable=<span class="string">'v5_xxxxxx'</span>,user=<span class="string">'xxxxx'</span>, password=<span class="string">'xxxxxx'</span>).load()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>df.take(<span class="number">3</span>)</div></pre></td></tr></table></figure></p>
<h1 id="二、Spark_Streaming">二、Spark Streaming</h1><p>　　Spark Streaming是Spark核心组件对在线流数据处理的扩展。在内部，Spark接受在线输入流数据，将其分成batches，然后再将结果输出，其提供了一个高层次抽象的DStream，表示为一种持续的输入流，其输入可以为（Kafka、Flume、Kinesis……反正我也不懂……）以及其他的DStream，在内部，DStream表示为一系列的RDDs构成。</p>
<h2 id="2-1_一个简单的例子，统计词的个数">2.1 一个简单的例子，统计词的个数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</div><div class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    sc = SparkContext(<span class="string">"local[2]"</span>， appName=<span class="string">"PythonStreamingNetworkWordCount"</span>)</div><div class="line">    ssc = StreamingContext(sc, <span class="number">1</span>)  <span class="comment"># 1s batch interval，为应用需求延迟和系统节点资源的权衡</span></div><div class="line"></div><div class="line">    lines = ssc.socketTextStream(<span class="string">"localhost"</span>,<span class="number">9999</span>) <span class="comment"># 建立Dstream，从TCP获得数据</span></div><div class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))\  <span class="comment">#flatMap，一对多，一个句子分多个词</span></div><div class="line">                  .map(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))\</div><div class="line">                  .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</div><div class="line">    counts.pprint()   <span class="comment"># 打印最开始的10个元素</span></div><div class="line"></div><div class="line">    ssc.start()   <span class="comment"># 真正从此开始，开始接受输入并计算输出</span></div><div class="line">    ssc.awaitTermination() <span class="comment"># 等待结束（手动停止或者出错，调用streamingContext.stop()）</span></div></pre></td></tr></table></figure>
<p>从两外一个终端开启”nc -lk 9999”不断输入作为数据源；另外一个终端提交任务”PYSPARK_PYTHON=python3.4 bin/spark-submit ../wordc_net.py “。<br> 注意：<br>默认的Spark打印的是INFO，这时候屏幕会不断的滚动消息，将LogLevel设置为WARN才能看的清楚。<br>一旦sc调用start()，就不能添加或者设置新的流计算；<br>一旦sc停止，就不能被重新启动了；<br>同一时刻一个JVM只能有一个活动的sc；<br>当StreamingContext调用stop()的时候，默认也会将SparkContext停止掉，要想保持SparkContext活着，那么stop()的参数请将stopSparkContext设置为false；<br>一个SparkContext可以重用被创建多个StreamingContext，只要之前的StreamingContext关闭停止就可以了。</p>
<h2 id="2-2_Discretized_Streams_(DStreams,离散流)">2.2 Discretized Streams (DStreams,离散流)</h2><p>　　DStream作为Spark Streaming的抽象数据类型，其内部原理就是对于采集到的每隔一个batch interval时间间隔的数据，生成一个该阶段的RDD，然后，上层对于DStream的操作，实际上就映射到底层的RDD的操作了。<br>　　在一个程序中，可以创建多个input DStream，这些数据都会被同时接收和处理，但是需要考虑到实际executor的执行能力。还需要特别注意的是，一个input DStream接收数据是需要占用一个thread的，所以如果有n个输入，需要&gt;2n个执行单元(local[2n])，否则就只能收数据，而收到的数据没有线程来处理了。<br>　　除了上面的TCP输入，还支持textFileStream输入，Spark Streaming会监测指定的文件目录，对于目录中新增的文件（嵌套目录不支持），会被当作新的数据源被处理，但是不支持文件内容的修改和新增等，这就意味着Spark Streaming一旦监测到该新增文件，就只接受一次该文件，因此一般都是别的地方将文件生成后，移动到监测的目录中。由于这种方式不需要使用receiver，所以这种情况不用增加额外的线程数目。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">streamingContext.textFileStream(dataDirectory)</div></pre></td></tr></table></figure></p>
<h2 id="2-3_Transformations">2.3 Transformations</h2><p>　　和之前的支持差不多，这里只列出差异的函数。由于DStream底层就是RDD支撑的，所以这里函数和RDD函数之间的关系还是比较微妙的</p>
<table>
<thead>
<tr>
<th>术语</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>countByValue()</td>
<td style="text-align:center">计数，对于每个元素K，得到（K, Long)</td>
</tr>
<tr>
<td>transform(func)</td>
<td style="text-align:center">对源DStream的每个RDD使用func转换，生成新的DStream，比如增加过滤器等，十分的强大</td>
</tr>
<tr>
<td>updateStateByKey(func)</td>
<td style="text-align:center">通过func将上一个时刻DStream每个key的状态，更新为新的状态，如果上个状态没有该key，其值为None</td>
</tr>
</tbody>
</table>
<p>这里的例子，是动态的对输入流的词频进行累计统计操作<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span><span class="params">(newValues, runningCount)</span>:</span></div><div class="line">    <span class="keyword">if</span> runningCount <span class="keyword">is</span> <span class="keyword">None</span>:  <span class="comment">#如果之前不存在，初始值为0</span></div><div class="line">       runningCount = <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> sum(newValues, runningCount) </div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line"></div><div class="line">    sc = SparkContext(appName=<span class="string">"PythonStreamingNetworkWordCount"</span>)</div><div class="line">    ssc = StreamingContext(sc, <span class="number">1</span>)</div><div class="line">    ssc.checkpoint(<span class="string">"checkpoint"</span>) <span class="comment">#必须的</span></div><div class="line"></div><div class="line">    lines = ssc.socketTextStream(<span class="string">"localhost"</span>,<span class="number">9999</span>)</div><div class="line">    counts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))\</div><div class="line">                  .map(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))\</div><div class="line">                  .updateStateByKey(updateFunction)    <span class="comment">#更新在此                 </span></div><div class="line">    counts.pprint()</div><div class="line"></div><div class="line">    ssc.start()</div><div class="line">    ssc.awaitTermination()</div></pre></td></tr></table></figure></p>
<h2 id="2-4_窗口操作">2.4 窗口操作</h2><p>　　需要两个参数：窗口尺寸、滑动间隔，参数值都必须是batch interval的整数倍。其对应的接口函数在下面列出，其中第一个函数用于生成一个新的DStream，其余的都是原有RDD的聚合函数，只是作用于windows窗口模式下的多个RDD而已。</p>
<table>
<thead>
<tr>
<th>术语</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>window(windowLength, slideInterval)</td>
<td style="text-align:center">形成一个新的DStream</td>
</tr>
<tr>
<td>countByWindow(windowLength, slideInterval)</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>reduceByWindow(func, windowLength, slideInterval)</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])</td>
<td style="text-align:center">这个函数比较的厉害，有个反向函数invFunc，整个函数的意思就是对进入窗口的新数据用func，而对离开窗口的数据用invFunc</td>
</tr>
<tr>
<td>countByValueAndWindow(windowLength, slideInterval, [numTasks])</td>
</tr>
</tbody>
</table>
<p>　　还有，默认窗口操作的数据都是persist缓存的，而默认的StorageLevel是MEMORY_ONLY_SER，不需要开发者手动调用。对于网络传递进来的数据流，默认的存储是复制到两个不同的节点上以实现容错。</p>
<h2 id="2-5_DStream的输出操作">2.5 DStream的输出操作</h2><p>　　实际类似RDD的actions，一般是现实、保存数据到网络或者文件系统上，从而驱动了真正计算的执行（最后foreachRDD实际是func的操作驱动真正计算）<br>| 术语   | 说明  |<br>| — |:—–:|<br>| pprint() | 打印出最开始的10个元素，主要用于调试 |<br>| saveAsTextFiles(prefix, [suffix]) | 将DStream的数据保存为prefix-TIME_IN_MS[.suffix] |<br>| foreachRDD(func) | 针对每个RDD的操作，这个func是在driver中执行的~rdd.foreachPartition |</p>
<h2 id="2-6_Checkpointing">2.6 Checkpointing</h2><p>　　既然是流数据的，那么程序就理应全天候跑的，Checkpointing就是设计出来应对这种应用的容错机制(系统出错\JVM崩溃等)，用于带有容错机制的文件系统(如HDFS)中。Spark支持的Checkpointing包括</p>
<ul>
<li>Metadata Checkpointing<br>保存了包括创建streaming应用的配置、DStream的操作、正在排队还未完成计算的batches等。这是用于driver programer节点出错时候的恢复。</li>
<li>Data Checkpoint<br>保存生成的RDDs到存储系统中，主要应对的是有状态的transformation，因为要生成新的RDD需要之前的RDD。当然，随着系统的执行，这样的依赖链会越来愈长，所以系统会周期性的将带有状态的transformation的中间RDDs保存到存储系统中。<br>对于程序中用到updateStateByKey/reduceByKeyAndWindow(with inverse function)这类有状态的transformation，以及期望driver programer能从错误中恢复出来，那么就需要使用checkpointing机制。</li>
</ul>
<p>通过<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">streamingContext.checkpoint(checkpointDirectory)</div></pre></td></tr></table></figure></p>
<p>设置Checkpointing目录，可以开启checkpointing机制。然后通常是调用<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">context = StreamingContext.getOrCreate(checkpointDirectory, <span class="keyword">function</span>ToCreateContext)</div></pre></td></tr></table></figure></p>
<p>这个函数，当检测checkpointDirectory不存在的时候，说明是第一次执行，functionToCreateContext被调用，当driver programer出错而自动重新启动(由部署的deployment infrastructure支持)的时候，这个目录存在了，就不会再次调用functionToCreateContext函数来创建SparkContext和StreamingContext了。</p>
<h2 id="2-7_实时性">2.7 实时性</h2><p>　　总结来说，Spark Stream实际就是一个时间窗口内的RDD操作，然后通过增加各种函数来关联之前的数据，从本质上来说，算是一个大颗粒的周期性任务，如果时间间隔太大，延迟就严重；间隔太小，反复的提交调度任务，系统的吞吐量降低,负载也会加重。</p>
<h1 id="三、MLlib">三、MLlib</h1><p>　　包括常用的分类、聚类、回归等算法的实现。</p>
<h1 id="四、GraphX">四、GraphX</h1><p>　　这个可不是图形化现实，而是基于图的算法，比如大名鼎鼎的PageRank，此处暂且不论了。</p>
<h1 id="参考">参考</h1><ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="external">Spark SQL, DataFrames and Datasets Guide</a>   </li>
<li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html" target="_blank" rel="external">Spark Streaming Programming Guide</a>    </li>
<li><a href="http://spark.apache.org/docs/latest/mllib-guide.html" target="_blank" rel="external">Machine Learning Library (MLlib) Guide</a>   </li>
<li><a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html" target="_blank" rel="external">GraphX Programming Guide</a>   </li>
</ul>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/201603/learn-note-of-spark-(3)-relative-modules.html">Spark学习手册（三）：Spark模块学习摘读</a></p>
        <p><span>最后更新:</span>2016-12-18, 22:06:06</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/201603/learn-note-of-spark-(3)-relative-modules.html" title="Spark学习手册（三）：Spark模块学习摘读">https://taozj.org/201603/learn-note-of-spark-(3)-relative-modules.html</a>
            <span class="copy-path" data-clipboard-text="原文: https://taozj.org/201603/learn-note-of-spark-(3)-relative-modules.html　　作者: taozj" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 请勿以任何形式转载文章完整正文
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/201604/statistics-frequency-bayesian.html">
                    统计学之边角料——频率派和贝叶斯派
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/201603/learn-note-of-spark-(2)-read-official-doc.html">
                    Spark学习手册（二）：Spark官方手册读摘
                </a>
            </div>
        
    </nav>


  
  
     <! -- 添加捐赠图标 -->

    

    <div class ="post-donate">
      <div id="donate_board" class="donate_bar center">
        <a id="btn_donate" class="btn_donate" href="javascript:;" title="Donate 打赏"></a>
        <span class="donate_txt">
           &uarr;<br>
           
            欣赏此文？赏份狗粮！
                     
        </span>
        <br>
      </div>  
  
      <div id="donate_guide" class="donate_bar center hidden">
        
          <a href="/post_images/resources/alipay.png" title="支付宝打赏" class="fancybox" rel="article0"       style="float:left;margin-left:25%;margin-right:2px;">
          <img src="/post_images/resources/alipay.png" title="支付宝打赏" height="164px" width="164px">
          </a> 
          

      
       <a href="/post_images/resources/weixin.png" title="微信打赏" class="fancybox" rel="article0">
         <img src="/post_images/resources/weixin.png" title="微信打赏" height="164px" width="164px">
       </a>
      
    
      </div>
  
      <script type="text/javascript">
        document.getElementById('btn_donate').onclick = function(){
          $('#donate_board').addClass('hidden');
          $('#donate_guide').removeClass('hidden');
        }
      </script>
    </div>

    

<! -- 添加捐赠图标 -->


  
</article>





    
      <div class="duoshuo" id="comments">
    <!-- 多说评论框 start -->
    <div class="ds-thread" data-thread-key="201603/learn-note-of-spark-(3)-relative-modules.html" data-title="Spark学习手册（三）：Spark模块学习摘读" data-url="https://taozj.org/201603/learn-note-of-spark-(3)-relative-modules.html"></div>
    <!-- 多说评论框 end -->
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:"freesign"};
    (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        <!-- ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//127.0.0.1:4000/js/embed.js'; -->
        ds.src = '/js/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
    </script>
    <!-- 多说公共JS代码 end -->
</div>

    




    <div class="scroll" id="post-nav-button">
        
            <a href="/201604/statistics-frequency-bayesian.html" title="上一篇: 统计学之边角料——频率派和贝叶斯派">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/201603/learn-note-of-spark-(2)-read-official-doc.html" title="下一篇: Spark学习手册（二）：Spark官方手册读摘">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/201702/study-note-of-scalable-backend-(5)-message-queue.html">后台开发那些常用技术再次小结（五）：消息队列</a></li><li class="post-list-item"><a class="post-list-link" href="/201702/study-note-of-scalable-backend-(4)-cache.html">后台开发那些常用技术再次小结（四）：缓存部分</a></li><li class="post-list-item"><a class="post-list-link" href="/201702/cmake-cheatsheet.html">CMake工具使用手册</a></li><li class="post-list-item"><a class="post-list-link" href="/201702/study-note-of-scalable-backend-(3)-storage.html">后台开发那些常用技术再次小结（三）：存储部分</a></li><li class="post-list-item"><a class="post-list-link" href="/201702/study-note-of-scalable-backend-(2)-web-service.html">后台开发那些常用技术再次小结（二）：Web服务层</a></li><li class="post-list-item"><a class="post-list-link" href="/201702/study-note-of-scalable-backend-(1)-front.html">后台开发那些常用技术再次小结（一）：前端部分</a></li><li class="post-list-item"><a class="post-list-link" href="/201702/little-thoughts-at-the-begin-of-2017.html">2017年春节返途中的两三思考</a></li><li class="post-list-item"><a class="post-list-link" href="/201701/gnu-gdb-debug.html">GNU GDB调试手册</a></li><li class="post-list-item"><a class="post-list-link" href="/201701/https-principle.html">HTTPS原理简单介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/201701/rpc-principle-and-tips.html">RPC设计和使用中的一些杂谈</a></li><li class="post-list-item"><a class="post-list-link" href="/201701/learn-note-of-distributed-system-(6)-application.html">分布式系统入门笔记（六）：基于ZooKeeper的分布式系统的应用场景</a></li><li class="post-list-item"><a class="post-list-link" href="/201701/learn-note-of-distributed-system-(5)-zab-consensus.html">分布式系统入门笔记（五）：ZooKeeper之ZAB一致性协议</a></li><li class="post-list-item"><a class="post-list-link" href="/201701/blog-collection.html">【置顶】博客资源收录大全</a></li><li class="post-list-item"><a class="post-list-link" href="/201701/linux-performance-basic.html">Linux服务器的那些性能参数指标</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/cmd-tools-sed-awk.html">文本处理利器sed与awk使用总结</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/install-gentoo-root-zfs-on-macbookpro-2015-early.html">MacbookPro上基于ZFS的Gentoo双系统安装</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/learn-note-of-distributed-system-(4)-raft-consensus.html">分布式系统入门笔记（四）：Raft一致性算法</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/read-note-of-amazon-dynamo.html">Amazon Dynamo论文阅读笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/consistent-hashing.html">一致性hashing的原理解析</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/nginx-load-balancing.html">基于Nginx的软件负载均衡实现解读</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/study-conclusion-stage.html">【置顶】个人阶段性学习和规划总结(技能树)</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/design-patterns-(3)-behavioral.html">设计模式整理总结（三）：行为型模式</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/design-patterns-(2)-structural.html">设计模式整理总结（二）：结构型模式</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/http2-spec.html">HTTP/2协议规范和特性解读</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/learn-note-of-google-grpc.html">Google gRPC框架学习笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/201612/tcp-connection-keep-alive.html">网络开发中客户端连接保鲜机制的实现方法</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/design-patterns-(1)-creational.html">设计模式整理总结（一）：创建型模式</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/forkp-mulit-process-manage-framework.html">forkp多进程程序管理库的轮子</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/learn-note-of-distributed-system-(3)-see-paxos-from-phxpaxos.html">分布式系统入门笔记（三）：从PhxPaxos中再看Paxos协议工程实现</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/learn-note-of-tencent-libco-coroutine.html">腾讯libco协程库学习使用笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/libto-coroutine-library-base-on-boost-context2.html">基于Boost.Context2库的协程库轮子libto的设计与实现</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/learn-note-of-distributed-system-(2)-paxos-algorithm.html">分布式系统入门笔记（二）：Paxos算法介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/learn-note-of-distributed-system-(1)-abstraction-and-2PC-3PC.html">分布式系统入门笔记（一）：分布式系统基本概念和两三阶段提交</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/about-multi-process-thread-dev-manage.html">浅谈多进程程序的进程控制和管理方式</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/transform-work-env-to-mac-os.html">macOS新平台工作环境的设置和迁移</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/introduction-of-boost-context-and-new-coroutine-library.html">Boost.Context库简介及Boost.Coroutine协程使用方式</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/data-structure-and-algorithm-(4)-sort.html">数据结构和算法（四）：主流内排序算法</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/data-structure-and-algorithm-(3)-rbtree.html">数据结构和算法（三）：红黑二叉树</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/data-structure-and-algorithm-(2)-avl.html">数据结构和算法（二）：AVL自平衡二叉树</a></li><li class="post-list-item"><a class="post-list-link" href="/201611/data-structure-and-algorithm-(1)-hash.html">数据结构和算法（一）：hash散列容器</a></li><li class="post-list-item"><a class="post-list-link" href="/201610/talk-about-singleton.html">说说设计模式中的单例模式</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/basics-of-boost-asio-(3)-strand.html">Boost-Asio网络开发基础知识（三）：Strand序列化执行用户回调</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/basics-of-boost-asio-(2)-overview-of-the async-framework.html">Boost.Asio网络开发基础知识（二）：异步框架总览</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/basics-of-boost-asio-(1)-read-the-docs.html">Boost.Asio网络开发基础知识（一）：读读文档</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/cpp11-atomic-and-memory-model.html">C++11标准中的Atomic原子操作和内存模型</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/lockless-in-multi-thread.html">多线程开发中无锁队列的设计和实现</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/talk-about-io-seperation-design.html">开发中IO分离设计的重构杂谈</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/learn-note-of-protobuf.html">Google Protobuf数据交换格式的使用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/read-(linux-mulit-thread-server-develop).html">《Linux多线程服务端编程》读摘</a></li><li class="post-list-item"><a class="post-list-link" href="/201609/usage-of-boost-coroutine.html">Boost.Coroutine协程库的简单使用</a></li><li class="post-list-item"><a class="post-list-link" href="/201608/feeling-of-cpp-11-and-two-ticks.html">C++11新标准阶段性学习心得及两个小轮子分享</a></li><li class="post-list-item"><a class="post-list-link" href="/201608/some-refined-and-modification-this-site.html">关于近来本站点的一些修改和设置</a></li><li class="post-list-item"><a class="post-list-link" href="/201607/learn-note-of-boost-(1)-smart-ptr-memory-pool.html">Boost库学习笔记(一)：智能指针和内存池</a></li><li class="post-list-item"><a class="post-list-link" href="/201607/simple-digit-recong-base-on-tesseract.html">基于Tesseract的数字识别程序</a></li><li class="post-list-item"><a class="post-list-link" href="/201607/construct-running-close-of-tcp.html">TCP链接的建立和关闭过程</a></li><li class="post-list-item"><a class="post-list-link" href="/201607/design-and-impl-of-minicached-base-on-memcached.html">基于memcached原理实现的单机轻量级通用缓存库</a></li><li class="post-list-item"><a class="post-list-link" href="/201606/learn-note-of-libevent-(3)-internel-impl-and-framework.html">Libevent学习笔记（三）：内部实现原理初探</a></li><li class="post-list-item"><a class="post-list-link" href="/201606/port-expose-based-on-libevent-(4)-dns-proxy-support.html">基于Libevent转发的内网端口暴露（四）：添加DNS代理的功能</a></li><li class="post-list-item"><a class="post-list-link" href="/201606/read-(the-art-of-sober-thinking).html">读《清醒思考的艺术》感</a></li><li class="post-list-item"><a class="post-list-link" href="/201606/personalized-reading-based-on-content-recommendation-(2)-svd-impl.html">基于内容推荐的个性化阅读实现（二）：基于SVD的推荐算法</a></li><li class="post-list-item"><a class="post-list-link" href="/201606/port-expose-based-on-libevent-(3)-ss5-proxy-support.html">基于Libevent转发的内网端口暴露（三）：添加SS5代理功能</a></li><li class="post-list-item"><a class="post-list-link" href="/201606/port-expose-based-on-libevent-(2)-refined-and-improvement.html">基于Libevent转发的内网端口暴露（二）：优化重构</a></li><li class="post-list-item"><a class="post-list-link" href="/201606/port-expose-based-on-libevent-(1)-basic-impl.html">基于Libevent转发的内网端口暴露（一）：基本实现</a></li><li class="post-list-item"><a class="post-list-link" href="/201606/personalized-reading-based-on-content-recommendation-(1)-general-impl.html">基于内容推荐的个性化阅读实现（二）：基本实现</a></li><li class="post-list-item"><a class="post-list-link" href="/201605/principle-of-oauth2.html">互联网OAuth 2.0开放授权原理</a></li><li class="post-list-item"><a class="post-list-link" href="/201605/data-type-and-index-of-mysql-database.html">MySQL数据类型整理和索引介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/201605/auto-answer-recommend-conclusion.html">答案自动推荐模块项目小结</a></li><li class="post-list-item"><a class="post-list-link" href="/201605/learn-note-of-libevent-(2)-thread-pool-in-memcached.html">Libevent学习笔记（二）：Memcached中Libevent和线程池使用初探</a></li><li class="post-list-item"><a class="post-list-link" href="/201605/fastcgi-support-for-http-server-libmicrohttpd.html">对libmicrohttpd添加FastCGI协议支持</a></li><li class="post-list-item"><a class="post-list-link" href="/201605/learn-note-of-libevent-(1)-basic-usage.html">Libevent学习笔记（一）：基本使用</a></li><li class="post-list-item"><a class="post-list-link" href="/201605/usage-of-apache-lucy-fulltext-index.html">Apache Lucy的全文检索引擎的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/fulltext-search-based-on-lsi-lda.html">基于LSI/LDA的文本检索的原理和操作步骤</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/linux-env-program-(2)-difference-select-poll-epoll.html">Linux环境开发（二）：IO复用之select/poll/epoll之原理和差异分析</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/linux-env-program-(1)-async-blocking-io-model.html">Linux环境开发（一）：同异步、阻塞的IO模型相关问题</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/oversee-of-http-based-on-libmicrohttpd.html">基于libmicrohttpd的HTTP服务器初探</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/server-prog-port-from-windows-to-linux.html">Windows服务端程序向Linux移植经验总结</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/recommend-system-algorithm.html">推荐系统常用的推荐算法</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/recommend-system-cases.html">推荐系统的典型推荐案例</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/how-to-calc-svd.html">SVD的数学计算步骤</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/conclusion-of-machine-learning-algorithm.html">不带公式的机器学习算法整理</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/random-sampling-method.html">随机采样方法</a></li><li class="post-list-item"><a class="post-list-link" href="/201604/statistics-frequency-bayesian.html">统计学之边角料——频率派和贝叶斯派</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/learn-note-of-spark-(3)-relative-modules.html">Spark学习手册（三）：Spark模块学习摘读</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/learn-note-of-spark-(2)-read-official-doc.html">Spark学习手册（二）：Spark官方手册读摘</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/learn-note-of-spark-(1)-construct-spark-hdfs.html">Spark学习手册（一）：HDFS支撑的Spark环境搭建与尝试</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/learn-note-of-pro-git.html">Pro Git速查笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/interesting-cases-of-machine-learning.html">深度学习的那些有趣案例</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/how-to-love-dog.html">我的养狗笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/one-year-old-of-my-zaizai.html">我们仔仔快要一岁啦</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/how-to-survive-from-info-ocean.html">面对越来越多的信息我们是怎么了</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/encrypt-safe-your-file-and-email-gpg.html">请善用GPG安全你的邮件和文件</a></li><li class="post-list-item"><a class="post-list-link" href="/201603/blog-site-under-https.html">我的博客用上HTTPS啦</a></li><li class="post-list-item"><a class="post-list-link" href="/201602/learn-note-of-numpy.html">NumPy科学计算库学习记录</a></li><li class="post-list-item"><a class="post-list-link" href="/201602/leave-tp-link-at-2015.html">2015年工作总结——离开TP之路</a></li><li class="post-list-item"><a class="post-list-link" href="/201602/read-(i-am-in-taobao-these-ten-years).html">读《淘宝技术这十年》感</a></li><li class="post-list-item"><a class="post-list-link" href="/201602/usage-and-optimize-of-zfs.html">终极文件系统ZFS的使用与优化技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/201602/setup-dev-env-for-pebble-smartphone.html">Pebble嵌入式开发环境搭建和开发测试样例</a></li><li class="post-list-item"><a class="post-list-link" href="/201601/read-(the-mystery-of-silicon-valley).html">读《硅谷之谜》感</a></li><li class="post-list-item"><a class="post-list-link" href="/201601/conclusion-of-machine-learning.html">机器学习中自然语言处理之总结</a></li><li class="post-list-item"><a class="post-list-link" href="/201601/gentoo-overlay-and-software-recommend.html">我的Gentoo Overlay和Linux软件推荐</a></li><li class="post-list-item"><a class="post-list-link" href="/201510/docker-examples.html">Docker容器技术使用实例</a></li><li class="post-list-item"><a class="post-list-link" href="/201510/mail-server-based-on-postfix-dovecot.html">搭建基于Postfix和Dovecot的邮件服务器</a></li><li class="post-list-item"><a class="post-list-link" href="/201509/rpmbuild-package-example-(2).html">RedHat系列软件打包实例（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/201509/rpmbuild-package-example-(1).html">RedHat系列RPM软件打包实例（一）</a></li><li class="post-list-item"><a class="post-list-link" href="/201509/generate-makefile-through-autotools.html">利用autotools自动生成项目的Makefile</a></li><li class="post-list-item"><a class="post-list-link" href="/201508/keepass-pass-save.html">开源的个人密码管理器软件KeePass</a></li><li class="post-list-item"><a class="post-list-link" href="/201508/learning-and-debug-linux-kernel-under-windows.html">在Windows下通过虚拟机搭建Linux内核的学习和调试环境</a></li></ul>




    <script>
        
    </script>

</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2015-2017 taozj
            </div>
            <div class="footer-right">
                <a href="http://www.miitbeian.gov.cn/" target="_blank" rel="nofollow">
                    <img src="/post_images/icons/icp_min.png" alt="粤ICP备17002382号-1">粤ICP备17002382号-1
                </a>
                <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258402767'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/stat.php%3Fid%3D1258402767%26online%3D2' type='text/javascript'%3E%3C/script%3E"));
                </script>
                Powered by <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>, theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题">Yelee</a>  enhanced by Nicol <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
    </div>
</footer>

    </div>
    
    <script src="/js/GithubRepoWidget.js"></script>

<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.1.22/require.min.js"></script>
<script src="/js/search.js"></script> 


    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 10;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>




    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

    <script src="//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js"></script>


   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</script>

  </div>
</body>
</html>